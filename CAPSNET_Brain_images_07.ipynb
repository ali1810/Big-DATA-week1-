{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CAPSNET_Brain_images_07",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1pCTjTzCSAGjNE1Ex5KWg87_OVAF3vmOq",
      "authorship_tag": "ABX9TyPadJLtZyNGlI6Jt0hEifXq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ali1810/Big-DATA-week1-/blob/master/CAPSNET_Brain_images_07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xohpjD8_elsH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "d3b655a3-0513-41fa-c2c1-56084d049986"
      },
      "source": [
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb5CI8NqemrT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "9d9e7d01-bdde-4c03-deb6-5d478a2bd09a"
      },
      "source": [
        "X = tf.placeholder(shape=[None, 28, 28, 1], dtype=tf.float32, name=\"X\")\n",
        "\n",
        "caps1_n_maps = 32\n",
        "caps1_n_caps = caps1_n_maps * 6 * 6  # 1152 primary capsules\n",
        "caps1_n_dims = 8\n",
        "\n",
        "conv1_params = {\n",
        "    \"filters\": 256,\n",
        "    \"kernel_size\": 9,\n",
        "    \"strides\": 1,\n",
        "    \"padding\": \"valid\",\n",
        "    \"activation\": tf.nn.relu,\n",
        "}\n",
        "\n",
        "conv2_params = {\n",
        "    \"filters\": caps1_n_maps * caps1_n_dims, # 256 convolutional filters\n",
        "    \"kernel_size\": 9,\n",
        "    \"strides\": 2,\n",
        "    \"padding\": \"valid\",\n",
        "    \"activation\": tf.nn.relu\n",
        "}\n",
        "\n",
        "conv1 = tf.layers.conv2d(X, name=\"conv1\", **conv1_params)\n",
        "conv2 = tf.layers.conv2d(conv1, name=\"conv2\", **conv2_params)\n",
        "\n",
        "caps1_raw = tf.reshape(conv2, [-1, caps1_n_caps, caps1_n_dims],\n",
        "                       name=\"caps1_raw\")\n",
        "\n",
        "def squash(s, axis=-1, epsilon=1e-7, name=None):\n",
        "    with tf.name_scope(name, default_name=\"squash\"):\n",
        "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
        "                                     keep_dims=True)\n",
        "        safe_norm = tf.sqrt(squared_norm + epsilon)\n",
        "        squash_factor = squared_norm / (1. + squared_norm)\n",
        "        unit_vector = s / safe_norm\n",
        "        return squash_factor * unit_vector\n",
        "\n",
        "caps1_output = squash(caps1_raw, name=\"caps1_output\")\n",
        "\n",
        "caps2_n_caps = 2\n",
        "caps2_n_dims = 16\n",
        "\n",
        "init_sigma = 0.1\n",
        "\n",
        "W_init = tf.random_normal(\n",
        "    shape=(1, caps1_n_caps, caps2_n_caps, caps2_n_dims, caps1_n_dims),\n",
        "    stddev=init_sigma, dtype=tf.float32, name=\"W_init\")\n",
        "W = tf.Variable(W_init, name=\"W\")\n",
        "\n",
        "batch_size = tf.shape(X)[0]\n",
        "W_tiled = tf.tile(W, [batch_size, 1, 1, 1, 1], name=\"W_tiled\")\n",
        "\n",
        "caps1_output_expanded = tf.expand_dims(caps1_output, -1,\n",
        "                                       name=\"caps1_output_expanded\")\n",
        "caps1_output_tile = tf.expand_dims(caps1_output_expanded, 2,\n",
        "                                   name=\"caps1_output_tile\")\n",
        "caps1_output_tiled = tf.tile(caps1_output_tile, [1, 1, caps2_n_caps, 1, 1],\n",
        "                             name=\"caps1_output_tiled\")\n",
        "\n",
        "caps2_predicted = tf.matmul(W_tiled, caps1_output_tiled,\n",
        "                            name=\"caps2_predicted\")\n",
        "\n",
        "# Dynamic Routing algorithm\n",
        "# Round 1\n",
        "raw_weights = tf.zeros([batch_size, caps1_n_caps, caps2_n_caps, 1, 1],\n",
        "                       dtype=np.float32, name=\"raw_weights\")\n",
        "routing_weights = tf.nn.softmax(raw_weights, dim=2, name=\"routing_weights\")\n",
        "\n",
        "weighted_predictions = tf.multiply(routing_weights, caps2_predicted,\n",
        "                                   name=\"weighted_predictions\")\n",
        "weighted_sum = tf.reduce_sum(weighted_predictions, axis=1, keep_dims=True,\n",
        "                             name=\"weighted_sum\")\n",
        "caps2_output_round_1 = squash(weighted_sum, axis=-2,\n",
        "                              name=\"caps2_output_round_1\")\n",
        "\n",
        "caps2_output_round_1_tiled = tf.tile(\n",
        "    caps2_output_round_1, [1, caps1_n_caps, 1, 1, 1],\n",
        "    name=\"caps2_output_round_1_tiled\")\n",
        "\n",
        "agreement1 = tf.matmul(caps2_predicted, caps2_output_round_1_tiled,\n",
        "                      transpose_a=True, name=\"agreement1\")\n",
        "# Round 2\n",
        "# Routing weight update\n",
        "raw_weights_round_2 = tf.add(raw_weights, agreement1,\n",
        "                             name=\"raw_weights_round_2\")\n",
        "routing_weights_round_2 = tf.nn.softmax(raw_weights_round_2,\n",
        "                                        dim=2,\n",
        "                                        name=\"routing_weights_round_2\")\n",
        "weighted_predictions_round_2 = tf.multiply(routing_weights_round_2,\n",
        "                                           caps2_predicted,\n",
        "                                           name=\"weighted_predictions_round_2\")\n",
        "weighted_sum_round_2 = tf.reduce_sum(weighted_predictions_round_2,\n",
        "                                     axis=1, keep_dims=True,\n",
        "                                     name=\"weighted_sum_round_2\")\n",
        "caps2_output_round_2 = squash(weighted_sum_round_2,\n",
        "                              axis=-2,\n",
        "                              name=\"caps2_output_round_2\")\n",
        "caps2_output_round_2_tiled = tf.tile(\n",
        "    caps2_output_round_2, [1, caps1_n_caps, 1, 1, 1],\n",
        "    name=\"caps2_output_round_2_tiled\")\n",
        "\n",
        "agreement2 = tf.matmul(caps2_predicted, caps2_output_round_2_tiled,\n",
        "                      transpose_a=True, name=\"agreement2\")\n",
        "\n",
        "# Round 3\n",
        "# Routing weight update\n",
        "raw_weights_round_3 = tf.add(raw_weights_round_2, agreement2,\n",
        "                             name=\"raw_weights_round_3\")\n",
        "routing_weights_round_3 = tf.nn.softmax(raw_weights_round_3,\n",
        "                                        dim=2,\n",
        "                                        name=\"routing_weights_round_3\")\n",
        "weighted_predictions_round_3 = tf.multiply(routing_weights_round_3,\n",
        "                                           caps2_predicted,\n",
        "                                           name=\"weighted_predictions_round_3\")\n",
        "weighted_sum_round_3 = tf.reduce_sum(weighted_predictions_round_3,\n",
        "                                     axis=1, keep_dims=True,\n",
        "                                     name=\"weighted_sum_round_3\")\n",
        "caps2_output_round_3 = squash(weighted_sum_round_3,\n",
        "                              axis=-2,\n",
        "                              name=\"caps2_output_round_3\")\n",
        "caps2_output_round_3_tiled = tf.tile(\n",
        "    caps2_output_round_3, [1, caps1_n_caps, 1, 1, 1],\n",
        "    name=\"caps2_output_round_3_tiled\")\n",
        "\n",
        "agreement3 = tf.matmul(caps2_predicted, caps2_output_round_3_tiled,\n",
        "                      transpose_a=True, name=\"agreement3\")\n",
        "\n",
        "# Round 4\n",
        "# Routing weight update\n",
        "raw_weights_round_4 = tf.add(raw_weights_round_3, agreement3,\n",
        "                             name=\"raw_weights_round_4\")\n",
        "routing_weights_round_4 = tf.nn.softmax(raw_weights_round_4,\n",
        "                                        dim=2,\n",
        "                                        name=\"routing_weights_round_4\")\n",
        "weighted_predictions_round_4 = tf.multiply(routing_weights_round_4,\n",
        "                                           caps2_predicted,\n",
        "                                           name=\"weighted_predictions_round_4\")\n",
        "weighted_sum_round_4 = tf.reduce_sum(weighted_predictions_round_4,\n",
        "                                     axis=1, keep_dims=True,\n",
        "                                     name=\"weighted_sum_round_4\")\n",
        "caps2_output_round_4 = squash(weighted_sum_round_4,\n",
        "                              axis=-2,\n",
        "                              name=\"caps2_output_round_4\")\n",
        "\"\"\"caps2_output_round_4_tiled = tf.tile(\n",
        "    caps2_output_round_4, [1, caps1_n_caps, 1, 1, 1],\n",
        "    name=\"caps2_output_round_4_tiled\")\n",
        "\n",
        "agreement4 = tf.matmul(caps2_predicted, caps2_output_round_4_tiled,\n",
        "                      transpose_a=True, name=\"agreement3\")\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "caps2_output = caps2_output_round_4\n",
        "\n",
        "def safe_norm(s, axis=-1, epsilon=1e-7, keep_dims=False, name=None):\n",
        "    with tf.name_scope(name, default_name=\"safe_norm\"):\n",
        "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
        "                                     keep_dims=keep_dims)\n",
        "        return tf.sqrt(squared_norm + epsilon)\n",
        "\n",
        "y_proba = safe_norm(caps2_output, axis=-2, name=\"y_proba\")\n",
        "\n",
        "\n",
        "y_proba_argmax = tf.argmax(y_proba, axis=2, name=\"y_proba\")\n",
        "\n",
        "y_pred = tf.squeeze(y_proba_argmax, axis=[1,2], name=\"y_pred\")\n",
        "\n",
        "y = tf.placeholder(shape=[None], dtype=tf.int64, name=\"y\")\n",
        "\n",
        "m_plus = 0.9\n",
        "m_minus = 0.1\n",
        "lambda_ = 0.5\n",
        "\n",
        "T = tf.one_hot(y, depth=caps2_n_caps, name=\"T\")\n",
        "\n",
        "caps2_output_norm = safe_norm(caps2_output, axis=-2, keep_dims=True,\n",
        "                              name=\"caps2_output_norm\")\n",
        "\n",
        "present_error_raw = tf.square(tf.maximum(0., m_plus - caps2_output_norm),\n",
        "                              name=\"present_error_raw\")\n",
        "present_error = tf.reshape(present_error_raw, shape=(-1, 2),\n",
        "                           name=\"present_error\")\n",
        "present_error\n",
        "\n",
        "absent_error_raw = tf.square(tf.maximum(0., caps2_output_norm - m_minus),\n",
        "                             name=\"absent_error_raw\")\n",
        "absent_error = tf.reshape(absent_error_raw, shape=(-1, 2),\n",
        "                          name=\"absent_error\")\n",
        "\n",
        "L = tf.add(T * present_error, lambda_ * (1.0 - T) * absent_error,\n",
        "           name=\"L\")\n",
        "\n",
        "margin_loss = tf.reduce_mean(tf.reduce_sum(L, axis=1), name=\"margin_loss\")\n",
        "\n",
        "mask_with_labels = tf.placeholder_with_default(False, shape=(),\n",
        "                                               name=\"mask_with_labels\")\n",
        "\n",
        "reconstruction_targets = tf.cond(mask_with_labels, # condition\n",
        "                                 lambda: y,        # if True\n",
        "                                 lambda: y_pred,   # if False\n",
        "                                 name=\"reconstruction_targets\")\n",
        "\n",
        "reconstruction_mask = tf.one_hot(reconstruction_targets,\n",
        "                                 depth=caps2_n_caps,\n",
        "                                 name=\"reconstruction_mask\")\n",
        "\n",
        "reconstruction_mask_reshaped = tf.reshape(\n",
        "    reconstruction_mask, [-1, 1, caps2_n_caps, 1, 1],\n",
        "    name=\"reconstruction_mask_reshaped\")\n",
        "\n",
        "caps2_output_masked = tf.multiply(\n",
        "    caps2_output, reconstruction_mask_reshaped,\n",
        "    name=\"caps2_output_masked\")\n",
        "\n",
        "decoder_input = tf.reshape(caps2_output_masked,\n",
        "                           [-1, caps2_n_caps * caps2_n_dims],\n",
        "                           name=\"decoder_input\")\n",
        "\n",
        "n_hidden1 = 512\n",
        "n_hidden2 = 1024\n",
        "n_output = 28 * 28\n",
        "\n",
        "with tf.name_scope(\"decoder\"):\n",
        "    hidden1 = tf.layers.dense(decoder_input, n_hidden1,\n",
        "                              activation=tf.nn.relu,\n",
        "                              name=\"hidden1\")\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2,\n",
        "                              activation=tf.nn.relu,\n",
        "                              name=\"hidden2\")\n",
        "    decoder_output = tf.layers.dense(hidden2, n_output,\n",
        "                                     activation=tf.nn.sigmoid,\n",
        "                                     name=\"decoder_output\")\n",
        "\n",
        "X_flat = tf.reshape(X, [-1, n_output], name=\"X_flat\")\n",
        "squared_difference = tf.square(X_flat - decoder_output,\n",
        "                               name=\"squared_difference\")\n",
        "reconstruction_loss = tf.reduce_mean(squared_difference,\n",
        "                                    name=\"reconstruction_loss\")\n",
        "\n",
        "alpha = 0.0005\n",
        "\n",
        "loss = tf.add(margin_loss, alpha * reconstruction_loss, name=\"loss\")\n",
        "\n",
        "\n",
        "correct = tf.equal(y, y_pred, name=\"correct\")\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer()\n",
        "training_op = optimizer.minimize(loss, name=\"training_op\")\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-30-d9e33cb66a21>:23: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-30-d9e33cb66a21>:32: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From <ipython-input-30-d9e33cb66a21>:67: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "WARNING:tensorflow:From <ipython-input-30-d9e33cb66a21>:226: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPizXZxvfZbN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8754c3b2-2267-48a0-cede-3b37b4aff242"
      },
      "source": [
        "# Preparing the dataset .......\n",
        "import numpy as np\n",
        "\n",
        "import os \n",
        "import cv2\n",
        "findpath= \"/content/drive/My Drive/train\"\n",
        "findpath1= \"/content/drive/My Drive/test\"\n",
        "normal_images=[]\n",
        "\n",
        "for i in os.listdir(findpath):\n",
        "    normal_images.append(os.path.join(findpath,i))\n",
        "\n",
        "image_arrays=[]\n",
        "\n",
        "for i in range(len(normal_images)):\n",
        "    img=cv2.imread(normal_images[i])\n",
        "    img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) ## for B&W  image use GRAY2BGR\n",
        "    img =cv2.resize(img,(28,28)) \n",
        "    image_arrays.append(img)\n",
        "x_train = np.array(image_arrays)\n",
        "print(x_train.shape)  \n",
        "test_image=[]\n",
        "\n",
        "for i in os.listdir(findpath1):\n",
        "    test_image.append(os.path.join(findpath1,i))\n",
        "cv2.imread(test_image[0]).shape\n",
        "image1_arrays=[]\n",
        "\n",
        "for i in range(len(test_image)):\n",
        "    try:\n",
        "        img=cv2.imread(test_image[i])\n",
        "        img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "        img =cv2.resize(img,(28,28)) \n",
        "        image1_arrays.append(img)\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "x_test = np.array(image1_arrays)\n",
        "print(x_test.shape)  \n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(87, 28, 28)\n",
            "(20, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyPnMazzhQC9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "68b5f48e-50dc-4380-b00b-63b703e59fe4"
      },
      "source": [
        "n_samples = 10\n",
        "\n",
        "plt.figure(figsize=(n_samples * 2, 3))\n",
        "for index in range(5,n_samples):\n",
        "    plt.subplot(1, n_samples, index + 1)\n",
        "    sample_image = x_train[index].reshape(28, 28)\n",
        "    plt.imshow(sample_image, cmap=\"binary\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x216 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5kCejPOiHN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epochs = 10\n",
        "batch_size = 5\n",
        "restore_checkpoint = False\n",
        "n_iterations_per_epoch = len(x_train) // batch_size\n",
        "n_iterations_validation = len(x_test) // batch_size\n",
        "best_loss_val = np.infty\n",
        "best_loss_val = np.infty\n",
        "checkpoint_path = \"./my_capsule_network0\"\n",
        "y_train=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "         0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "len(y_train)  \n",
        "y_test=[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1]       "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAcNGVEoi9l2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "f67af96e-2042-4f99-ebcb-dadc7cc4383d"
      },
      "source": [
        "init=tf.global_variables_initializer()\n",
        "n_epochs = 10\n",
        "batch_size = 5\n",
        "restore_checkpoint = False\n",
        "n_iterations_per_epoch = len(x_train) // batch_size\n",
        "n_iterations_validation = len(x_test) // batch_size\n",
        "best_loss_val = np.infty\n",
        "checkpoint_path = \"./my_capsule_network0\"\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    if restore_checkpoint and tf.train.checkpoint_exists(checkpoint_path):\n",
        "        saver.restore(sess, checkpoint_path)\n",
        "    else:\n",
        "        init.run()\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for iteration in range(1, n_iterations_per_epoch + 1):\n",
        "            X_batch = x_train[(iteration-1)*batch_size:(iteration*batch_size),:]\n",
        "            y_batch = y_train[(iteration-1)*batch_size:(iteration*batch_size)]\n",
        "            # Run the training operation and measure the loss:\n",
        "            _, loss_train = sess.run([training_op, loss],feed_dict={X: X_batch.reshape([-1, 28, 28, 1]),y: y_batch,\n",
        "                           mask_with_labels: True})\n",
        "            print(\"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n",
        "                      iteration, n_iterations_per_epoch,\n",
        "                      iteration * 100 / n_iterations_per_epoch,\n",
        "                      loss_train),\n",
        "                  end=\"\")\n",
        "\n",
        "        # At the end of each epoch,  \n",
        "        # measure the validation loss and accuracy:\n",
        "        loss_vals = []\n",
        "        acc_vals = []\n",
        "        for iteration in range(1, n_iterations_validation + 1):\n",
        "            X_batch = x_test[(iteration-1)*batch_size:(iteration*batch_size),:]\n",
        "            y_batch = y_test[(iteration-1)*batch_size:(iteration*batch_size)]\n",
        "            loss_val, acc_val = sess.run(\n",
        "                    [loss, accuracy],\n",
        "                    feed_dict={X: X_batch.reshape([-1, 28, 28, 1]),\n",
        "                               y: y_batch})\n",
        "            loss_vals.append(loss_val)\n",
        "            acc_vals.append(acc_val)\n",
        "            print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
        "                      iteration, n_iterations_validation,\n",
        "                      iteration * 100 / n_iterations_validation),\n",
        "                  end=\" \" * 10)\n",
        "        loss_val = np.mean(loss_vals)\n",
        "        acc_val = np.mean(acc_vals)\n",
        "        print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}{}\".format(\n",
        "            epoch + 1, acc_val * 100, loss_val,\n",
        "            \" (improved)\" if loss_val < best_loss_val else \"\"))\n",
        "\n",
        "        # And save the model if it improved: \n",
        "        \n",
        "        if loss_val < best_loss_val:\n",
        "            save_path = saver.save(sess, checkpoint_path)\n",
        "            best_loss_val = loss_val\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  Val accuracy: 50.0000%  Loss: 2.506116 (improved)\n",
            "Epoch: 2  Val accuracy: 50.0000%  Loss: 2.506952\n",
            "Epoch: 3  Val accuracy: 50.0000%  Loss: 2.506325\n",
            "Epoch: 4  Val accuracy: 50.0000%  Loss: 2.500095 (improved)\n",
            "Epoch: 5  Val accuracy: 50.0000%  Loss: 2.502954\n",
            "Epoch: 6  Val accuracy: 50.0000%  Loss: 2.507764\n",
            "Epoch: 7  Val accuracy: 50.0000%  Loss: 2.507989\n",
            "Epoch: 8  Val accuracy: 50.0000%  Loss: 2.507997\n",
            "Epoch: 9  Val accuracy: 50.0000%  Loss: 2.507972\n",
            "Epoch: 10  Val accuracy: 50.0000%  Loss: 2.507936\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}